{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch\n",
    "from tqdm.auto import tqdm\n",
    "#from dotenv import load_dotenv\n",
    "\n",
    "#from db import init_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_url = \"https://github.com/DataTalksClub/llm-zoomcamp/blob/main/04-monitoring/data/results-gpt4o-mini.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'{github_url}?raw=1'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_llm</th>\n",
       "      <th>answer_orig</th>\n",
       "      <th>document</th>\n",
       "      <th>question</th>\n",
       "      <th>course</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You can sign up for the course by visiting the...</td>\n",
       "      <td>Machine Learning Zoomcamp FAQ\\nThe purpose of ...</td>\n",
       "      <td>0227b872</td>\n",
       "      <td>Where can I sign up for the course?</td>\n",
       "      <td>machine-learning-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You can sign up using the link provided in the...</td>\n",
       "      <td>Machine Learning Zoomcamp FAQ\\nThe purpose of ...</td>\n",
       "      <td>0227b872</td>\n",
       "      <td>Can you provide a link to sign up?</td>\n",
       "      <td>machine-learning-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes, there is an FAQ for the Machine Learning ...</td>\n",
       "      <td>Machine Learning Zoomcamp FAQ\\nThe purpose of ...</td>\n",
       "      <td>0227b872</td>\n",
       "      <td>Is there an FAQ for this Machine Learning course?</td>\n",
       "      <td>machine-learning-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The context does not provide any specific info...</td>\n",
       "      <td>Machine Learning Zoomcamp FAQ\\nThe purpose of ...</td>\n",
       "      <td>0227b872</td>\n",
       "      <td>Does this course have a GitHub repository for ...</td>\n",
       "      <td>machine-learning-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To structure your questions and answers for th...</td>\n",
       "      <td>Machine Learning Zoomcamp FAQ\\nThe purpose of ...</td>\n",
       "      <td>0227b872</td>\n",
       "      <td>How can I structure my questions and answers f...</td>\n",
       "      <td>machine-learning-zoomcamp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          answer_llm  \\\n",
       "0  You can sign up for the course by visiting the...   \n",
       "1  You can sign up using the link provided in the...   \n",
       "2  Yes, there is an FAQ for the Machine Learning ...   \n",
       "3  The context does not provide any specific info...   \n",
       "4  To structure your questions and answers for th...   \n",
       "\n",
       "                                         answer_orig  document  \\\n",
       "0  Machine Learning Zoomcamp FAQ\\nThe purpose of ...  0227b872   \n",
       "1  Machine Learning Zoomcamp FAQ\\nThe purpose of ...  0227b872   \n",
       "2  Machine Learning Zoomcamp FAQ\\nThe purpose of ...  0227b872   \n",
       "3  Machine Learning Zoomcamp FAQ\\nThe purpose of ...  0227b872   \n",
       "4  Machine Learning Zoomcamp FAQ\\nThe purpose of ...  0227b872   \n",
       "\n",
       "                                            question  \\\n",
       "0                Where can I sign up for the course?   \n",
       "1                 Can you provide a link to sign up?   \n",
       "2  Is there an FAQ for this Machine Learning course?   \n",
       "3  Does this course have a GitHub repository for ...   \n",
       "4  How can I structure my questions and answers f...   \n",
       "\n",
       "                      course  \n",
       "0  machine-learning-zoomcamp  \n",
       "1  machine-learning-zoomcamp  \n",
       "2  machine-learning-zoomcamp  \n",
       "3  machine-learning-zoomcamp  \n",
       "4  machine-learning-zoomcamp  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.iloc[:300]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Getting the enbeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the embeddings for the first LLM answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_llm = df.iloc[0].answer_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the first value of the resulting vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.42244676"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_answer_v = embedding_model.encode(answer_llm)\n",
    "first_answer_v[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Computing the dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_llm = df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_llm': 'You can sign up for the course by visiting the course page at [http://mlzoomcamp.com/](http://mlzoomcamp.com/).',\n",
       " 'answer_orig': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork',\n",
       " 'document': '0227b872',\n",
       " 'question': 'Where can I sign up for the course?',\n",
       " 'course': 'machine-learning-zoomcamp'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_llm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dot_product(record):\n",
    "    answer_orig = record['answer_orig']\n",
    "    answer_llm = record['answer_llm']\n",
    "\n",
    "    v_orig = embedding_model.encode(answer_orig)\n",
    "    v_llm = embedding_model.encode(answer_llm)\n",
    "\n",
    "    return v_orig.dot(v_llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02acb037c4b4cf6b811518ae035c96d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluations = [] \n",
    "\n",
    "for record in tqdm(answers_llm):\n",
    "    dot_product = compute_dot_product(record)\n",
    "    evaluations.append(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17.515995,\n",
       " 13.418406,\n",
       " 25.31325,\n",
       " 12.147421,\n",
       " 18.747725,\n",
       " 33.970398,\n",
       " 30.251698,\n",
       " 29.52158,\n",
       " 35.2722,\n",
       " 27.751759]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the 75% of the score? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    300.000000\n",
       "mean      27.495996\n",
       "std        6.384743\n",
       "min        4.547925\n",
       "25%       24.307844\n",
       "50%       28.336862\n",
       "75%       31.674305\n",
       "max       39.476013\n",
       "Name: dot_product, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dot_product'] = evaluations \n",
    "df['dot_product'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75% of the score is 31.67430543899536\n"
     ]
    }
   ],
   "source": [
    "print(f\"75% of the score is {df['dot_product'].describe()['75%']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Computing the cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize a vector \n",
    "def normalize_vector(v):\n",
    "    norm = np.sqrt((v * v).sum())\n",
    "    v_norm = v / norm \n",
    "    return v_norm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dot_product_normalized(record):\n",
    "    answer_orig = record['answer_orig']\n",
    "    answer_llm = record['answer_llm']\n",
    "\n",
    "    v_orig = embedding_model.encode(answer_orig)\n",
    "    v_llm = embedding_model.encode(answer_llm)\n",
    "\n",
    "    v_orig_normalized = normalize_vector(v_orig)\n",
    "    v_llm_normalized = normalize_vector(v_llm)\n",
    "    \n",
    "    return v_orig_normalized.dot(v_llm_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706eb64017044d278fa8890e62c962e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "normalized_evaluations = [] \n",
    "\n",
    "for record in tqdm(answers_llm):\n",
    "    n_dot_product = compute_dot_product_normalized(record)\n",
    "    normalized_evaluations.append(n_dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5067542,\n",
       " 0.38854897,\n",
       " 0.7185991,\n",
       " 0.33726645,\n",
       " 0.5217922,\n",
       " 0.83053195,\n",
       " 0.7462834,\n",
       " 0.6944062,\n",
       " 0.8468864,\n",
       " 0.65590733]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_evaluations[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the 75% cosine in the scores? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    300.000000\n",
       "mean       0.728392\n",
       "std        0.157755\n",
       "min        0.125357\n",
       "25%        0.651273\n",
       "50%        0.763761\n",
       "75%        0.836235\n",
       "max        0.958796\n",
       "Name: n_dot_product, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['n_dot_product'] = normalized_evaluations \n",
    "df['n_dot_product'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75% of cosine scores is 0.8362347185611725\n"
     ]
    }
   ],
   "source": [
    "print(f\"75% of cosine scores is {df['n_dot_product'].describe()['75%']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Rouge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_llm': \"Yes, all sessions are recorded, so if you miss one, you won't miss anything. You can catch up on the content later. Additionally, you can submit your questions in advance for office hours, and those sessions are also recorded.\",\n",
       " 'answer_orig': 'Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.',\n",
       " 'document': '5170565b',\n",
       " 'question': 'Are sessions recorded if I miss one?',\n",
       " 'course': 'machine-learning-zoomcamp'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_llm[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's compute the ROUGE score between the answers at the index 10 of our dataframe (doc_id=5170565b)\n",
    "record = answers_llm[10]\n",
    "scores = rouge_scorer.get_scores(record['answer_llm'], record['answer_orig'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rouge-1': {'r': 0.45454545454545453, 'p': 0.45454545454545453, 'f': 0.45454544954545456}, 'rouge-2': {'r': 0.21621621621621623, 'p': 0.21621621621621623, 'f': 0.21621621121621637}, 'rouge-l': {'r': 0.3939393939393939, 'p': 0.3939393939393939, 'f': 0.393939388939394}}]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the F score for rouge-1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-score for rouge-1 = 0.45454544954545456 \n"
     ]
    }
   ],
   "source": [
    "print(f\"F-score for rouge-1 = {scores[0]['rouge-1']['f']} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Average Rouge Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a rouge score compute the average of all 3 scores \n",
    "def average_rouge_score(rouge_score_record):\n",
    "    rouge_1 = rouge_score_record['rouge-1']['f']\n",
    "    rouge_2 = rouge_score_record['rouge-2']['f']\n",
    "    rouge_l = rouge_score_record['rouge-l']['f']\n",
    "    rouge_avg = (rouge_1 + rouge_2 + rouge_l) / 3\n",
    "    return rouge_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average between rouge-1, rouge-2 and rouge-l for the same record from Q4: 0.35490034990035496\n"
     ]
    }
   ],
   "source": [
    "# Let's compute the average between rouge-1, rouge-2 and rouge-l for the same record from Q4\n",
    "avg_score = average_rouge_score(scores[0])\n",
    "print(f\"average between rouge-1, rouge-2 and rouge-l for the same record from Q4: {avg_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Average rouge score for all the data points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321c00606f6343cd8ae6208baa93c29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_scores = []\n",
    "for record in tqdm(answers_llm):\n",
    "    score = rouge_scorer.get_scores(record['answer_llm'], record['answer_orig'])\n",
    "    avg = average_rouge_score(score[0])\n",
    "    avg_scores.append(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg = pd.DataFrame({'avg_rouge_scores': avg_scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_rouge_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.072882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.091435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.327658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.150821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.098731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg_rouge_scores\n",
       "0          0.072882\n",
       "1          0.091435\n",
       "2          0.327658\n",
       "3          0.150821\n",
       "4          0.098731"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_rouge_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.313205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.158133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.197358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.298640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.404169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.813043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       avg_rouge_scores\n",
       "count        300.000000\n",
       "mean           0.313205\n",
       "std            0.158133\n",
       "min            0.000000\n",
       "25%            0.197358\n",
       "50%            0.298640\n",
       "75%            0.404169\n",
       "max            0.813043"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_avg.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rouge score across all the records is 0.313205367339838\n"
     ]
    }
   ],
   "source": [
    "# What's the average rouge_l across all the nodes? \n",
    "print(f\"Average rouge score across all the records is {df_avg['avg_rouge_scores'].mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
